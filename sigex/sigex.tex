%: ----------------------- sigex file header -----------------------
\chapter{Signal Extraction}

% the code below specifies where the figures are stored
\ifpdf
    \graphicspath{{sigex/figures/PNG/}{sigex/figures/PDF/}{sigex/figures/}}
\else
    \graphicspath{{sigex/figures/EPS/}{sigex/figures/}}
\fi

% ----------------------------------------------------------------------
% ----------------------- sigex content -------------------------
% ----------------------------------------------------------------------

Solar neutrino events are monte-carlo simulated using ``RAT'', a Geant-4 based
simulation and analysis toolkit.
RAT simulates all effects after the initial interaction, including photon
propagation and detection, and particle scattering.
Beyond the photon and physics simulation RAT also simluates the SNO+ DAQ and trigger electronics,
allowing the effects of digitization and electronics noise to be simulated.

A solar neutrino production rate is an input to the simulation.
A cross-section model take from \cite{bahcall} and \cite{lookitup}
is used to estimate the elastic scattering interaction rate.
RAT provides an accurate model of the detector response for each interaction.

Solar neutrino events are simulated on run-by-run basis with a fixed average rate
of interactions.
Each run's simulation is matched to the detector trigger and
daq settings for that run. To ensure adequate monte-carlo statistics the
rate of solar $\nu_{e}$ and $\nu_{\mu\text{,\,}tau}$ interactions is artificially
enhanced by a factor of XXX and XXX; the enhanced rate is later removed as a correction
to the normalization of the PDFs created from the monte-carlo simulation.

$\theta_{sun}$ is defined by
\begin{equation}
\cos\theta_{sun} = \vec{d}\cdot\vec{d_{sun}}\text{, }
\end{equation}
where $\vec{d_{e}}$ represents the reconstructed direction of an event and
$\vec{d_{sun}}$ is the direction vector pointing from the center of the sun to
the reconstructed position of the event.
$\vec{d_{sun}}$ estimates the direction the neutrino was travelling when it interacted
within the detector, $\vec{d_{\nu}}$; it is assumed the neutrino travelled directly
from the center of the sun without scattering off anything while it travelled.
This is a good assumption because the neutrino cross-section is small that it's very
unlikely the neutrino will interact with anything before interacting in the detector.
Assuming the neutrino comes from the center of the sun is a poor assumption for an individual
neutrino, but averaged over many neutrinos it is a good assumption.
Additionally, correcting for the radius the neutrino is produced at would only adjust
the direction by at most $0.1\deg$.
Figure XXX shows the angle between $d_{sun}$ and $d_{\nu}$ for simulated solar neutrino
events.

Figure XXX shows why $\theta_{sun}$ is a useful variable for a solar neutrino analysis.
By comparing the rates of events with different values for $\theta_{sun}$ one can
extract a background rate and a solar rate.

\section{Reconstruction}
A series of reconstruction algorithms are ran over all events that pass data cleaning.
These algorithms estimate the position, time, direction, and energy of the event.
All events are reconstructed under the hypothesis that the PMT hits are from cherekov radiation
produced by a single electron.
Additionally, the reconstruction algorithms use only the hits in the prompt time window to ensure only light
that travelled directly from the event origin is used.
The same reconstruction algorithms are used on both simulated and detected events.

The direction ($\vec{d}$), time ($t_{0}$), and poition ($\vec{p}$) are determined by performing a likelihood
fit to the time and position of PMT hits.
The algorithm evaluates the likelihood of a hypothesized event position and time by
calculating the time residual for each hit PMT,
\begin{equation}
    \label{eqn:tres}
t_{res} = t_{PMT} - t_{transit} - t_{0}
\end{equation}
and using a PDF for $t_{res}$ determined from simulation, $P(t_{res})$.
The position and time that minimize the quantity
\begin{equation}
\sum_{i=0}^{N_{PMT}} P(t_{res}) % TODO need to refactor this to include the index i
\end{equation}
is used as the event position and time.
The direction is determined by evaluating $\theta_{PMT}$ for each hit where
$\theta_{PMT}$ is defined by,
\begin{equation}
    \cos\theta_{PMT} = \vec{d}\cdot\left(\vec{p}_{PMT} - \vec{p}\right)\text{.}
\end{equation}
The likelihood, $P(\theta_{PMT})$, is determined from simulation, the direction
that minimizes
\begin{equation}
\sum_{i=0}^{N_{PMT}} P(\theta_{PMT})
\end{equation}
is used as the reconstructed event direction.

The kinetic energy of the event is determined separately
using the best fit position, and time as an input.
The position and time are used to determine the number of PMT
hits that occurred in a prompt 18\,ns window.
Then the number of photons that would most likely produce
that number of PMT hits is estimated using a combination of
analytic calculation and monte-carlo simulation.
A look up table is used to estimate the most likely electron
kinetic energy that would produce the determined number of photons.

\subsection{ITR}
The time residual, defined in equation{eqn:tres} for a PMT hit is an extremely useful quantity because in
general light that travels directly from an interaction will have a very small
time residual.
Light that is produced by another source, or reflects off of a detector component
between production and detection will have a larger time residual.

The fraction of hits that satisify
\begin{equation}
    -4 > t_{res} < 9 %TODO CHECK VALUES
\end{equation}
is known as the ``In-time ratio'' (ITR).
The expected distrubtion in ITR for electrons is shown in figure XXX.

\subsection{$\beta_{14}$}
The quantity $\beta_{14}$ is used to quantify how isotropic the hits
in an event is. It is defined as
% TODO check this is correct
\begin{equation}
    \beta_{14} = \sum_{j=0}^{i}\sum_{i=0}^{i=N_{PMT}} P_{1}(\cos(\theta_{ij})) + P_{4}(\cos(\theta_{ij}))
\end{equation}

% TODO, check this as well
The quantity $\theta_{ij}$ is the angle subtended by the vectors pointing
from the reconstructed position of the event to the $i^{\text{th}}$ and
$j^{\text{th}}$ hit PMT.

The expected distribution of $\beta_{14}$ for electron events within the
detector volume is shown in Figure XXX.

%The fraction of hits that appear in a very prompt time window is a useful
%value for removing instrumental backgrounds. Cherenkov radiation produces
%a very short pulse of light within the detector, so any source of light
%that has a longer time profile is unlikely to be from a Cherenkov process.
%This is quantified using the ``In-time ratio'' (ITR) for an event.
%ITR is the fraction of light with a time residual.
\section{Calibration}
The accuracy of simulated events is evaluated with data taken while a
radioactive source was deployed within the detector volume.
For this analysis was an $\ce{^{16}N}$ source was used.

The $\ce{^{16}N}$ source was developed by $SNO$, it uses a commercial
deuterium and tritium generator (DT-generator) to produce gaseous $\ce{^{16}N}$.
The gas is pumped into the deployed source where it can undergo $\beta$-decay
to an exciteed state of $\ce{^{16}O}$, the $\ce{^{16}O}$ will then de-excite
and typically emit a $XXX$\,MeV gamma particle. Higher energy gammas are emitted
at a lower rate, the branching ratios for the de-excitation gammas are shown in
figure XXX.

A small block of plastic scintillator, observed by a PMT, is embedded within the
source cannister. That PMT can detect the $\beta$ from the initial
$\ce{^{16}N} \rightarrow \ce{^{16}O}$ decay. That PMT signal is used as a
tag in the detector DAQ to identify events from the deployed source.

The source position within the AV was varied in a 3-dimensional
scan.
A 1-dimensional scan was done along the z-axis outside the AV volume,
but inside the PSUP, as well.
Scanning many positions allowed for a position dependent evaluation of
systematics.

\subsection{Energy Calibration}

The relative energy scale $\delta_E$ and detector energy resolution $\sigma$
are determined by fitting the reconstructed energy spectrum $P(T_\mathrm{e})$
with the predicted apparent energy spectrum $P_\mathrm{source}(T_e)$ convolved
with a Gaussian~\cite{Dunford:2006qb}:
\begin{equation}
  P(T_\mathrm{e}) = N \int P_\mathrm{source}(T_e)\frac{1}{\sqrt{2\pi}\sigma}e^-{\frac{((1+\delta_E)T_\mathrm{e}-T\prime_{e})^2}{2\sigma^2}}dT\prime_{e}.%-\Delta E
\label{eq:convolution}
\end{equation}
Here, $\sigma$ is the detector-only resolution, which excludes the contribution
from Cherenkov photo-statistics and the production of electrons from the initial
$\gamma$'s, because these are accounted for in $P_\mathrm{source}(T_e)$.  The
detector resolution is expected to be dominated by a $\sqrt{E}$ term, owing to
variations in light collection and photo-electron production among PMTs,
yielding $\sigma(E) = b\sqrt{E}$, where $b$ is to be fit.  Other terms are
expected to contribute much less to the resolution, and for robustness, it is
preferred to have only one parameter to fit.  Fortunately, the choice of terms
in the resolution expression has little impact on the fit results.  An energy
offset or bias is not considered in the fit because
%all components of the energy reconstruction are multiplicative, and
it is assumed that there is no significant background contribution to the
measured signals.  The determination of $P_\mathrm{source}(T_{e})$ is discussed in
the next section.




Figure~\ref{XXX} shows the  distribution of EnergyRSP reconstructed energy for
$\ce{^{16}}$ events, both simulated and detected.
\subsection{Direction Calibration}
\subsection{Position Calibration}
\section{Simulation}
\subsection{RAT}
A monte-carlo simulation of particle interactions in the detector is used
for predicting detector observables for events.
The simulation pacakage used is called RAT, it is a Geant4-based simulation that
contains a detector and DAQ simulation in addition to simulation of particle
interactions and photon propagation.

\subsection{Solar Neutrino Fluxes}
The expected spectral shape and normalization for the solar neutrino signal is
taken from the BS05(OP) standard solar model.

\subsection{Solar Neutrino Cross-sections}
The rate of solar neutrino for a given flux follows from the cross-section for
interaction. The only interaction relevant for the SNO+ detector is the
neutrino-electron elastic scattering interaction.
Neutrino-nuclear interactions occur as well, but they either cannot be identified
separately from the radioactive backgrounds, or they occur at a very small rate
compared to the elastic-scattering rate.

The cross-section for the elastic-scattering interaction is take from
XXX.
XXX Something about radiative corrections

\subsection{Survival Probability Simulation}
A simulation of the expected solar survival probability curve for any set of mixing
parameters was used. The survival probability is calculated using a
3-flavor adiabatic calculation. The calculation was developed by the
SNO collaboration \cite{XXX}.

is used to calculate the fraction of the solar neutrino flux that arrives
at Earth and interacts as a $\nu_{e}$ vs the fraction that is $\nu_{\mu}$ or $\nu_{\tau}$.

Simulated events are used to estimate the distribution of events in $\cos\theta_{sun}$ and
electron recoil energy $T_{e}$.

The simlated events are histogrammed to estimate the underlying PDFs
of observable reconstructed energy and event direction. Cut's are placed
on each event to
\section{Data Selection}
\subsection{Run Selection}
\subsection{Event Selection}

\subsection{Hit Cleaning}

\section{Livetime}
\section{Data Cleaning}
There are a number of instrumental effects that can cause an event
to be recorded by the detector, these events typically have some
sort of distinguishing feature or features that set them apart
from events that originate from particle interactions within the
detector.
A number of algorithms and cuts have been designed to identify and remove
these events from the dataset.
These algorithms are said to ``clean'' the data by removing events
of instrumental origin.

Events of instrumental origin are not well modeled within our simulation,
so it is not used for evaluating the efficiencies and sacrifices of
data cleaning cuts.
Instead a data-driven approach is used that relies primarily on calibration
data from the $\ce{^{16}N}$ source.

\subsection{Ped Cut}
During normal detector operations there are a few trigger calibration
tests that are periodically ran.
These tests use the PEDESTAL signal to inject a certain amount of fake hits
into the detector, and events with those hits are inspected to evaluate the efficiency and
quality of the trigger response.
It's very important that these events are clearly identified and removed from the
dataset so that the fake PEDESTAL hits are not confused for a real signal.
Additionally, the trigger calibration processes usually include changing settings
related to the PEDESTAL signal on the FEC, there's reason to believe these sort
of changes can introduce noise to the front-end.
So an aggressive approach of cutting all events that are within one second of
a pedestal event is used.
This not only cuts events but introduces a dead time into the dataset,
this deadtime is subtracted from the overall livetime.
\subsection{Flashers}
The primary type of instrumental event that must be removed is ``flashers'' and
``shark-fin'' events.
Both of these result from charge build-up on the PMT-base causing a
spark.
For a flasher event the light from the spark escpaes through the PMT face
and illuminates the PMTs on the other side of the detector.
Flashers ocurr at a rate of a few per minute.
A shark-fin is similar but the spark is either small enough or located
in a position such that the light does not escape the PMT.
In both types of events the PMT in which the spark ocurrs with readout
a very high-charge hit, and the channels next to it on the FEC will
have low-charge hits from electronic pickup.
For shark-fin event no other channels will be hit, except possibly by an
accidental coincidence; for a flasher hit a number of hits will
ocurr from the light that escaped the PMT.
Since the number of PMT hits that ocurr in a flasher event can vary quite wildy,
anywhere betweeen tens of hits and hundreds, they can reconstruct
to a wide range of energies and possibly contiminate a signal region.
So many of the data cleaning cuts are designed to ensure that all
flasher events are identfied and removed from the dataset.

\subsection{ZeroZero Cut}
The GTID for the FEC is stored in a ripple counter, it's often the case that
when the bottom two bits of the counter rollover the event that gets recorded in
the FEC memory gets corrupted.
When this happens the builder cannot put the corrupted hits into the event correctly,
and the hits will effectively be discarded.
This means that event the detectors effective photon detection efficiecy is lower
for events that have a GTID with $00$ in the bottom two bits.
Rather than correct for this inefficiecy in reconstruction, events with GTID
ending in $00$ are discarded. This corresponds to a random pre-scale
on our by a factor of $\frac{1}{256}$.
\subsection{Crate Isotropy Cut}
The Crate Isotropy Cut is designed to remove events that are isolated
in one or a few electronics crates.
Events originating from light within the detector are unlikely to have any
preference in electronics space.
However hits caused by electrical noise that was created near the electronics
can show a very distinct preference for one crate.
The criteria for this cut is that fraction of hits in any single
is greater that $70\%$ and that the fractions of hits within
that crate are either $80\%$ within adjacent FECs or $70\%$ within
adjacent channels.
\subsection{Flasher Geometry Cut}
\subsection{ITC Timespread Cut}
\subsection{Missed Muon Follower Cut}
\subsection{CAEN Cut}
I developed a new data cleaning cut, called the ``CAEN Cut'', that follows from
the AMB Cut from SNO.
The AMB Cut attempted to remove events from flashers the dataset by requiring
that the integral and peak height of the ESUMH trigger sum (as measured by the
AMB) fall below some threshold value.
The CAEN Cut performs a similar function, it calculates the baseline subtracted
integral and peak height of the digitized ESUMH tigger signal and places a cut on those values.

The baseline value of each trace is calculated as the average value of the first
$20$ samples and the $65^{\text{th}}$ to $85^{\text{th}}$ samples.
I chose to use two windows, one before the trigger pulse, one after the trigger pulse, to
correct for any overall slope across the digtized window.
The CAEN window is $104$ samples long, the final 19 samples are not used because they often
include a large noise pulse.
The noise pulse comes from the GT pulse arriving at the front-end and generating electrical noise,
it's typically called ``readout noise''.
The readout noise makes the last $\approx20$ samples of the CAEN trace nearly
useless.

The determined baseline is subtracted from the CAEN trace and the integral and maximum
peak height are calculated from the samples between the two baseline windows.
To pass the CAEN Cut the peak and integral must fall between an upper and lower, nhit dependent,
cut cut value.
The cut values are given by
\begin{equation}
    f(n) = C\left(1-\sigma(n)\right) + \sigma(n)\left(mn+b\right)\text{.}
    \ref{eqn:cc_threshold}
\end{equation}
Here $\sigma(x)$ indicates a sigmoid function,
\begin{equation}
    \sigma(x) = \frac{1}{1+e^{\frac{-(x-x_{0})}{w}}}
\end{equation}
The cut values are meant to be constant value at lower nhit, and then
linear with nhit above $\approx15$ nhit, the sigmoid allows for a smooth
transition between those two functions; for both the upper and lower threshold
the sigmoid position ($x_{0}$) and width ($w$) are $15$\,nhit and $5$\,nhit respectively.
The constant value at lower nhit is $C$ the slope of the line at higer nhit
is given by $m$ and the value $b$ is required to be
\begin{equation}
    b = \frac{C}{mx_{0}}
\end{equation}
so that there is not discontinuity between the two cut regions.
The values for these parameters are given in Table~\ref{XXX}.

The reason for the two cut regions is because at lower nhit the signal
peak is smaller than the noise one the ESUMH signal, so the only requirement
is that the peak and integral be consistent with a noise only trigger sum.
At higher nhit the ESUMH signal scales linearly with nhit, each new hit
adds approximately the same amount of height to the trigger pulse.

The cut parameters were determined from two calibration datasets, the first was
tagged $\ce{^{16}N}$
events. The second was a sample of $PULSE\_GT$ triggers taken during normal
running.
The two datasets are used to determine the cut parameters for the two
different cut regions.
The $\ce{^{16}N}$ data was used to determine cut values for the higher nhit
region, the $PULSE\_GT$ data was used for the lower nhit cut values.

For both regions the value of the integral or peak height that include
99\% of the events at each nhit is found. Then the parameters of
Eqn.~\ref{eqn:cc_threshold} that best fit those points is determined.
Then Eqn.~\ref{eqn:cc_threshold} with the best fit upper and lower
parameters to include 99\% of the calibration data become the threshold
values for rejecting flasher events.
The 99\% criteria was chosen arbitarily to ensure that the fraction of
``good'' events rejected by this cut was similar to that of other data
cleaning cuts.
Figure~\ref{XXX} shows how the ESUMH CAEN trace integral is distributed
in the two calibration datasets and for standard physics data taking.

\section{Systematics}
Systematics associated with event reconstruction, livetime, mixing parameters,
and trigger efficiency are considered for this analysis.
The event reconstruction systematics are uncertainties on the energy reconstruction
scale and resolution, position reconstruction resolution and scale, and the
resolution of the direction reconstruction.

These systematics are generally treated in the same, or a similar, way, to evaluate
the effect they have on the flux result.
The uncertainties on each quantity are determined from a separate analysis, \textit{e.g.}
from an analysis $\ce{^{16}N}$ data.
Those uncertainties are propagated through the analysis by modifying the relevant quantities
according to the one-$\sigma$ uncertainty.
The PDFs that result from the modified events are used in the analysis to extract
a modified result, that modified result is taken as the one-$\sigma$ effect of the
systematic on the result.
This process is detailed more for each considered uncertainty below.

\subsection{Energy Resolution}
The energy resolution uncertainty is determined primarily from the $\ce{16}N$ analysis.
The resolution was determined to be XXX. %TODO
The reconstructed energy of the MC simulated events is mapped to a Gaussian distribution with a variance given
by
\begin{equation}
  \sigma^{2} = \sigma_{E}^{2}\left(\left(1 + \delta_{E}\right)^2 - 1\right)\text{.}
  \label{eqn:systmatic_esmear}
\end{equation}
This process of mapping a single energy value to a Gaussian distribution is referred to as ``smearing''.
Here $\sigma_{E}$ is given by $\sqrt{E}$ to match the functional form used in the fit for the systematics,
Eqn~\ref{XXX}.
The idea behind this smearing is to compensate for the possibility that our monte-carlo simulation could have
a systematically smaller energy resolution than occurs in real data.
So by applying a smearing the monte-carlo energy resolution is artificially deteriorated, and the uncertainty
on the resolution is accounted for.
A similar process does not exists to account for the possibility that the monte-carlo simulation has a poorer
energy resolution than data taken from the real detector; there's no way to ``un-smear'' the reconstructed MC
event energy.
So, to account the effect of an over-estimated energy resolution the error on the result is assumed to be symmetric.
As a penalty for this assumption the larger uncertainty between the positive and negative uncertainty on the
energy resolution is used.

\subsection{Energy Scale}
The energy scale systematic uncertainty
\subsection{Fiducial Volume}
Uncertainty on the fiducial
\subsection{Angular Resolution}
\subsection{Mixing Parameters}
\subsection{Trigger Efficiency}
\subsection{Livetime}
